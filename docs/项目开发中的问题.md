[TOC]









## 数据集问题

文本中出现太多重复句子，影响事件模版推导的效果

<img src="/Users/skipper/Library/Application Support/typora-user-images/image-20200924213100220.png" alt="image-20200924213100220" style="zoom:50%;" />

<img src="/Users/skipper/Library/Application Support/typora-user-images/image-20200928210318600.png" alt="image-20200928210318600" style="zoom:50%;" />


数据集文章的问题：
    1. 包含太多重复文章 ---》 simhash去重
    2. 存在质量很低的文章 ---》 如何过滤
    3. 存在对事件模版推导有很大影响但是没有意义的文章，例如：多个句子一直重复 ---》如何过滤



不是所有热搜单个热搜来搜索相关文章来执行事件模版的推导吧：
    1. 热搜没有意义  ---》 如何判断一个热搜在事件模版推导上有没有意义 
    2. 单个热搜的粒度太小，不能代表一个事件的类型。 ---》如何用热搜表示一个类型的事件
    
    
换一个想法：就是目的不是做事件模版的推导（不是泛化定义的模版）， 而是做热搜话题模版的推导（但是利用事件模版推导的方法）。    

https://leetcode-cn.com/problems/construct-binary-tree-from-inorder-and-postorder-traversal/





## openIE问题

中文相关的open IE系统比较少，后序的event schema induction该如何怎么做？



1. OpenIE的问题：就是对于提取得到的由动能动词构成的关系元组，实际上对事件模版推导有用的就是动能动词的宾语而不是动能动词本身，例如：进行+访问，有意义的是访问而不是进行。 

   ---》 问题：如何对这种情况进行判别，1.人工构建一个汉语的动能动词词表，而是对于这种结构提取的时候进行标注。

2. 由于OpenIE是对每对儿实体对儿进行relationship word判断，如果在长文本中，一些指代词都不会被提取出来作为实体（即使提取出来也没有意义），例如：李克强来到我的家乡。这里家乡通过指代消解提前预处理替换掉就可以提取得到（李克强，来，邯郸）。

3. 本质同2，只是**句子级别**的关系提取，能否扩展到文档级别。

   使用指代消解方法

4. 特定实体类型丢失了很多元组信息

   通过抽取特定pos的实体，构建实体对，抽取更有意义的信息。虽然可以这样，但是会丢失很多信息，例如：“赵丽颖生下儿子。“，但是如果对所有名词n对进行关系提取的话，很可能造成很多重复的元组信息，---》 如何对重复的元组进行过滤又是一个问题？

5. 11.04 多个抽取范式的过程中都对entity执行了偏差修正，即check_entity()，造成代码重复，

   Solution：在extract_by_dsnf.py中写一个方法，独立实现候选实体对的偏差修正，并记录偏差修正结果。



### OpenIE中特点

通过抽取特定pos的实体，构建实体对，抽取更有意义的信息。

```
通过只抽取与某些指定类型的实体：人名，机构名，地名等，---》过滤掉很多无意义的信息
例如：习近平主席和李克强总理接见普京。 --》实体：['习近平', '李克强', '普京']，不会单独的寻找'总统'相关的信息
```

虽然可以这样，但是会丢失很多信息，例如：“赵丽颖生下儿子。“，但是如果对所有名词n对进行关系提取的话，很可能造成很多重复的元组信息，---》 如何对重复的元组进行过滤又是一个问题？





## 转移到LTP4的问题

1. 名词无法合并，只能根据分词得到的结果作为每个词。

   导致的问题：根据提取范式，可能提取出多个相同意思的triple

   <img src="/Users/skipper/Library/Application Support/typora-user-images/image-20201103154544714.png" alt="image-20201103154544714" style="zoom: 33%;" />

   因为在提取 同济 时，会做偏差修正，从而将 同济 补全为 同济大学

2. nertag问题

   LTP4标注得到的是一个词区间，需要分配。

   会发生多个word组成一个命名实体：例如下图：

   <img src="/Users/skipper/Library/Application Support/typora-user-images/image-20201103155248255.png" alt="image-20201103155248255" style="zoom: 33%;" />

   复旦 和 大学 两个token构成一个NE 复旦大学。

   这个也仅仅是少例：对于人名和地名一般不会出现拆分的情况，应该只有机构名分词后出现被拆分的情况。

   （这里本质是由分词模型造成的，而LTP4是使用transfomer中加载深度模型分词的，为了得到词的hidden只能替换效果更好的深度模型，不能使用pkuseg分词器结果作为输入。）

   --->突然感觉：如果后期放宽实体的词性类型，可能不只有 机构名 实体需要这样处理，还包括一些名词修饰名词的情况，从而在放宽实体的词性类型的情况下不会重复提取元组。

   

   例子： 奥巴马总统毕业于哈弗大学。

   ​	分词： 奥巴马 总统 毕业 于 哈弗 大学 。

   ​	句法： 主语：总统， 宾语：大学

   Solution：不需要特别的针对 机构实体， 只需要对实体的偏正部分的token进行标记即可，从而避免提取重复的关系元组。

   通过对实体对（奥巴马，哈佛）提取时，会将 总统 和 大学 标记，从而不会被再次提取。

   错---------  美国 和 英国 有一个没有提取。。。。。。

3. 



## 语言知识

<img src="/Users/skipper/Library/Application Support/typora-user-images/image-20201105161421150.png" alt="image-20201105161421150" style="zoom:33%;" />





```
entity_object = self.search_entity(ent2.head_word) 就是ent1

SBV_VOB(entity1, entity_object, entity_coo=entity2, entity_flag='object')
```



## 问题和改进

1. 元组泛化时，实体类型需要扩充（英文使用WordNet的语义类别）
2. 英文使用正则表达式抽取 数字 和 日期， 这里可以吗？？？



### 2020.11.11:

#### 可能需要

	1. 是否要对分析的句子长度加上限制
 	2. 考虑一下，如果要将关系元组用于事件模版推导中，是否要去掉第一个语义范式抽取的元组，因为第一个范式抽取到的都是修饰的关系元组。

#### 问题

<img src="/Users/skipper/Library/Application Support/typora-user-images/image-20201111102024244.png" alt="image-20201111102024244" style="zoom:25%;" />![image-20201111102043883](/Users/skipper/Library/Application Support/typora-user-images/image-20201111102043883.png)<img src="/Users/skipper/Library/Application Support/typora-user-images/image-20201111102024244.png" alt="image-20201111102024244" style="zoom:25%;" />![image-20201111102043883](/Users/skipper/Library/Application Support/typora-user-images/image-20201111102043883.png)

1. 无脑的从es中拿出召回的文章，导致后面的文章和搜索的phrase不相关。（如上图）
2. 计算dist不能根据Triple在list中的下标，因为泛化之后的元组和原始元组数属于同一个？？？？还是怎么计算。？？？？(已解决)

### 2020.11.17

1. 将各种log输出到文件中去，文件路径问题（已解决：使用绝对路径）
2. 会不会由于只是根据检索词条检索出来的文章，导致很多描述同一事件的文章没有检索出来/检索的文章少，导致得到的触发词有限。
3. 参数问题：
   1. 取前多少个seeds
   2. 每个seed取前多少个相关triples
4. 计算速度太慢





## 



保留之前每个文档中分析的得到的triple，然后另外得到一个unique的triples，之后再遍历每个文档计算triple和triple之间的权重。



其实如果返回多维的Triple list就不需要记录Triple的idx_sentence,idx_document字段，但是为了方便后面的计算，还是记录较好。







## 换方向

### 2020.11.19

- 问题

  搜索召回相关性问题：通过ES检索解决，不实用 match_pharse，而是对输入的topic首先进行分词，在进行es检索时，对分词之后的每个词都要在title/content必须全部出现，

  可以对extract()函数，设定多个参数来实现不同的检索功能。

- TODO

  通过openIE分析检索得到的文章，进行word graph的构建，前端进行展示。

  - 需要重新构建索引，在前端进行展示的时候，需要通过点击某个词，并把相关的文章id/url显示出来，问了避免再次从mongo中拿，需要在es中存储文章的url，
  - 目前openIE抽取得到的tirple都有对应的articleID和sentenceID标示，后期对于前端显示不知道有没有用处。

效果：

​	由于目前设定分词之后的每个词都必须在title中出现，因此搜索的越详细，返回的文章越少，但是对于之后的搜索，因为是对某个word进行检索，因此返回的文章比较多。



```python
    def __init__(self, id, url, title, content, time, media_show, media_level, qscore, thumb, score):
        self.id = id
        self.url = url
        self.title = title
        self.content = content
        self.time = time
        self.media_show = media_show
        self.media_level = media_level
        self.qscore = qscore
        self.thumb = thumb
        self.score = score
```



### 2020.11.23

pkuseg词性标注

<img src="/Users/skipper/Library/Application Support/typora-user-images/image-20201123111926434.png" alt="image-20201123111926434" style="zoom:50%;" />



```python
allow_pos = [
        'n', 'i', 'l', 'j', 'nr', 'nz', 'ns', 'v', 't', 's', 'nt', 'vn', 'vx', 'an'
    ]
    keywords = []
    # for word in jieba.cut(topic):
    #     keywords.append(word)
    # for word, flag in pseg.cut(topic):
    #     if flag in allow_pos:
    #         keywords.append(word)
    seg = pkuseg.pkuseg(model_name='news', postag=True)
    for item in seg.cut(topic):
        if item[1] in allow_pos:
            keywords.append(item[0])
```

在前端展示提取的阿斗的所有triples









心得：

开发过程中，实际的效果和理想的差距很大，可能在很多环节都出现问题，有可能数据集有问题，有可能方法的效果不好等等，但是只要一个pipeline中某个环节效果不好就会影响到之后的所有环节的效果都比较差。



学到知识：

1. logging模块配置
2. 类作为key，`__hash__, __eq__`
3. 











